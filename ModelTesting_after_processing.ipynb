{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["E7XcL25YyqYc","i-A1RkiG1JoN"],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13920312,"sourceType":"datasetVersion","datasetId":8870219}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classification power and AD with varying jet $p_T$ thresholds\n\nIn this notebook we take a preprocessed file and test the how well can the data be classified by varying minimum jet $p_T$ thresholds. The plots get stored in a folder in google drive.","metadata":{"id":"ZTQMkUnLIAav"}},{"cell_type":"code","source":"# Standard library\nimport os\nimport math\nimport json\nimport pickle\nfrom datetime import datetime\n\n# Third-party scientific stack\nimport numpy as np\nimport pandas as pd\nimport itertools\n\n# Plotting\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Graph utilities\nimport networkx as nx\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.decomposition import PCA\n\n# Dimensionality reduction\n# import umap\n\n# Colab drive (only needed if you actually run in Google Colab)\n# from google.colab import drive\n# drive.mount('/content/drive')\n","metadata":{"id":"YuU0HDKZeweY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b28e80c-2f32-43a3-e961-49816c31a46d","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:36:45.201438Z","iopub.execute_input":"2025-11-29T13:36:45.201598Z","iopub.status.idle":"2025-11-29T13:36:49.072361Z","shell.execute_reply.started":"2025-11-29T13:36:45.201582Z","shell.execute_reply":"2025-11-29T13:36:49.071761Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"All pytorch related imports done in the section GNN supervised Classification","metadata":{"id":"zwiOXigV_Kx-"}},{"cell_type":"code","source":"# Set plotting style at module level\nplt.rcParams.update({\n    # Font sizes\n    'font.size': 18,\n    'axes.labelsize': 18,\n    'axes.titlesize': 18,\n    'xtick.labelsize': 16,\n    'ytick.labelsize': 16,\n    'legend.fontsize': 16,\n    'legend.frameon': False,  # No box around legend\n    'axes.grid': False,\n    # Tick settings\n    'xtick.direction': 'in',\n    'ytick.direction': 'in',\n    'xtick.major.size': 10,\n    'ytick.major.size': 10,\n    'xtick.minor.size': 5,\n    'ytick.minor.size': 5,\n    'xtick.major.width': 1,\n    'ytick.major.width': 1,\n    'xtick.top': True,\n    'ytick.right': True,\n    'xtick.minor.visible': True,\n    'ytick.minor.visible': True\n})","metadata":{"id":"6h74igsd8tiP","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:36:49.073763Z","iopub.execute_input":"2025-11-29T13:36:49.074144Z","iopub.status.idle":"2025-11-29T13:36:49.078532Z","shell.execute_reply.started":"2025-11-29T13:36:49.074125Z","shell.execute_reply":"2025-11-29T13:36:49.077777Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"with open(\"/kaggle/input/balanced-with-dr-or/balanced_dfs_no_dup_OR.pkl\", \"rb\") as f:\n    ML_dict = pickle.load(f)\n","metadata":{"id":"bKCcKEStfZgk","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:36:49.079195Z","iopub.execute_input":"2025-11-29T13:36:49.079472Z","iopub.status.idle":"2025-11-29T13:36:52.686563Z","shell.execute_reply.started":"2025-11-29T13:36:49.079452Z","shell.execute_reply":"2025-11-29T13:36:52.685988Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Details of the data\nML_dict is a dictionary with the dataframes\n\n```\n'all_signals', 'HAHMggf', 'HNLeemu', 'HtoSUEP',\n'VBF_H125_a55a55_4b_ctau1_filtered', 'Znunu',\n'ggF_H125_a16a16_4b_ctau10_filtered', 'hh_bbbb_vbf_novhh_5fs_l1cvv1cv1'\n```\nAll of them have the same columns:\n\n```\n'j0pt', 'j0eta', 'j0phi', 'j1pt', 'j1eta', 'j1phi', 'j2pt', 'j2eta',\n       'j2phi', 'j3pt', 'j3eta', 'j3phi', 'j4pt', 'j4eta', 'j4phi', 'j5pt',\n       'j5eta', 'j5phi', 'e0pt', 'e0eta', 'e0phi', 'e1pt', 'e1eta', 'e1phi',\n       'e2pt', 'e2eta', 'e2phi', 'mu0pt', 'mu0eta', 'mu0phi', 'mu1pt',\n       'mu1eta', 'mu1phi', 'mu2pt', 'mu2eta', 'mu2phi', 'ph0pt', 'ph0eta',\n       'ph0phi', 'ph1pt', 'ph1eta', 'ph1phi', 'ph2pt', 'ph2eta', 'ph2phi',\n       'METpt', 'METeta', 'METphi', 'run_number', 'event_number', 'weight',\n       'target'\n```\nWhen loaded with `balanced_dfs_no_dup_processed.pkl`, the dataframes contain events for which there are no duplicate objects. Events with undefined METpt have been removed. All events where all objects have 0 pt have been removed. All of them have equal amount of signal and background ('target' == 'EB_test').\n\nWhen loaded with `balanced_dfs_no_dup_OR.pkl`, in addition to the above mentioned processing, overlap removal has also been performed.","metadata":{"id":"BZ6_B4Zj3cnb"}},{"cell_type":"markdown","source":"## Random Forest","metadata":{"id":"E7XcL25YyqYc"}},{"cell_type":"code","source":"# Consistent style\nplt.rcParams['figure.figsize'] = (8,6)\nplt.rcParams['font.size'] = 12\n\n# Jet-pt thresholds you want to test\nJET_PT_THRESHOLDS = [5, 15, 25, 45, 60, 80]\n\n# Which jet columns to use\nJET_PT_COLS = [f\"j{i}pt\" for i in range(6)]   # j0pt ... j5pt\n","metadata":{"id":"IhAB-ECaiMU_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PLOT_BASE_DIR = \"/content/drive/MyDrive/Datasets/plots_with_OR\"\nos.makedirs(PLOT_BASE_DIR, exist_ok=True)","metadata":{"id":"Qour2QBkH23S"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define all the necessary helper functions","metadata":{"id":"5u-wW8m4HDpA"}},{"cell_type":"code","source":"def apply_jet_pt_threshold(df, threshold):\n    \"\"\"\n    Returns a filtered dataframe where all nonzero jets have pt >= threshold.\n    Condition: for each jet jX,\n      keep event if (jXpt == 0) or (jXpt >= threshold)\n    \"\"\"\n    mask = np.ones(len(df), dtype=bool)\n    for col in JET_PT_COLS:\n        if col in df.columns:\n            mask &= (df[col] == 0) | (df[col] >= threshold)\n    return df[mask]\n","metadata":{"id":"3VYVeVs81ZUV"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_dataset(df):\n    \"\"\"\n    Drops unwanted columns, splits dataset once for reproducibility.\n    \"\"\"\n    # Features = all physics columns except bookkeeping\n    drop_cols = [\"run_number\", \"event_number\", \"target\", \"weight\"]\n    feature_cols = [c for c in df.columns if c not in drop_cols]\n\n    X = df[feature_cols].copy()\n    y = (df[\"target\"] == \"EB_test\").astype(int)\n\n    # Single train/test split to be reused for all thresholds\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, stratify=y, random_state=42\n    )\n\n    return X_train, X_test, y_train, y_test, feature_cols\n","metadata":{"id":"BgHV-0BJGrD0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_evaluate_rf(X_train, y_train, X_test, y_test):\n    \"\"\"Trains a simple, stable Random Forest and returns ROC curve + AUC.\"\"\"\n\n    rf = RandomForestClassifier(\n        n_estimators=300,\n        max_depth=None,\n        min_samples_split=2,\n        random_state=42,\n        n_jobs=-1\n    )\n    rf.fit(X_train, y_train)\n\n    # Probabilities for ROC\n    y_score = rf.predict_proba(X_test)[:, 1]\n\n    fpr, tpr, _ = roc_curve(y_test, y_score)\n    roc_auc = auc(fpr, tpr)\n\n    return rf, fpr, tpr, roc_auc\n","metadata":{"id":"IU6UK5Y1GrBP"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_roc_curves(roc_results, dataset_name, save_dir):\n    plt.figure()\n    for T, (fpr, tpr, roc_auc) in roc_results.items():\n        plt.plot(fpr, tpr, label=f\"T={T} GeV (AUC={roc_auc:.3f})\")\n\n    # plt.plot([0,1], [0,1], \"k--\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(f\"ROC Curves — {dataset_name}\")\n    plt.legend()\n    plt.grid(True)\n\n    plt.savefig(os.path.join(save_dir, \"roc_curves.png\"), dpi=200, bbox_inches=\"tight\")\n    plt.close()\n","metadata":{"id":"T-qGVeHoGq--"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_auc_vs_threshold(roc_results, dataset_name, save_dir):\n    thresholds = list(roc_results.keys())\n    auc_vals = [roc_results[T][2] for T in thresholds]\n\n    plt.figure()\n    plt.plot(thresholds, auc_vals, marker=\"o\")\n    plt.xlabel(\"Jet $p_T$ Threshold [GeV]\")\n    plt.ylabel(\"AUC\")\n    plt.title(f\"AUC vs Jet $p_T$ Threshold — {dataset_name}\")\n    plt.grid(True)\n\n    plt.savefig(os.path.join(save_dir, \"auc_vs_threshold.png\"), dpi=200, bbox_inches=\"tight\")\n    plt.close()\n","metadata":{"id":"XuNuzpE8GzMe"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_event_yields(counts_sig, counts_bkg, dataset_name, save_dir):\n    plt.figure()\n    plt.plot(list(counts_sig.keys()), list(counts_sig.values()),\n             marker=\"o\", label=\"Signal\")\n    plt.plot(list(counts_bkg.keys()), list(counts_bkg.values()),\n             marker=\"s\", label=\"Background\")\n\n    plt.xlabel(\"Jet $p_T$ Threshold [GeV]\")\n    plt.ylabel(\"Events passing selection\")\n    plt.title(f\"Event Yields — {dataset_name}\")\n    plt.legend()\n    plt.grid(True)\n\n    plt.savefig(os.path.join(save_dir, \"event_yields.png\"), dpi=200, bbox_inches=\"tight\")\n    plt.close()\n","metadata":{"id":"iupkPm-HGzKT"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compare_feature_importances(\n        rf_low, rf_high, feature_cols, dataset_name, save_dir,\n        low_T=15, high_T=60, topN=12):\n\n    importances_low = rf_low.feature_importances_\n    importances_high = rf_high.feature_importances_\n\n    idx = np.argsort(importances_high)[::-1][:topN]\n\n    plt.figure(figsize=(9,6))\n    plt.barh(\n        [feature_cols[i] for i in idx],\n        importances_high[idx],\n        alpha=0.7,\n        label=f\"T={high_T} GeV\"\n    )\n    plt.barh(\n        [feature_cols[i] for i in idx],\n        importances_low[idx],\n        alpha=0.7,\n        label=f\"T={low_T} GeV\"\n    )\n\n    plt.gca().invert_yaxis()\n    plt.xlabel(\"Feature Importance\")\n    plt.title(f\"Feature Importance Comparison — {dataset_name}\")\n    plt.legend()\n\n    plt.savefig(os.path.join(save_dir, \"feature_importances.png\"), dpi=200, bbox_inches=\"tight\")\n    plt.close()\n","metadata":{"id":"zFKooa141ZSL"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Main function","metadata":{"id":"KfMVfyuWHJk9"}},{"cell_type":"code","source":"def run_full_analysis(dataset_name):\n    print(f\"=== Running full analysis for: {dataset_name} ===\")\n\n    df = ML_dict[dataset_name].copy()\n\n    # Create directory for this dataset's plots\n    save_dir = os.path.join(PLOT_BASE_DIR, dataset_name)\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Prepare once\n    X_train_all, X_test_all, y_train_all, y_test_all, feature_cols = prepare_dataset(df)\n\n    roc_results = {}\n    counts_sig = {}\n    counts_bkg = {}\n    rf_models = {}\n\n    # Loop thresholds\n    for T in JET_PT_THRESHOLDS:\n        print(f\"\\n→ Applying jet pt threshold T = {T} GeV\")\n\n        X_train = apply_jet_pt_threshold(X_train_all, T)\n        y_train = y_train_all.loc[X_train.index]\n\n        X_test = apply_jet_pt_threshold(X_test_all, T)\n        y_test = y_test_all.loc[X_test.index]\n\n        counts_sig[T] = (y_test == 1).sum()\n        counts_bkg[T] = (y_test == 0).sum()\n\n        rf, fpr, tpr, roc_auc = train_and_evaluate_rf(\n            X_train, y_train, X_test, y_test\n        )\n\n        roc_results[T] = (fpr, tpr, roc_auc)\n        rf_models[T] = rf\n\n    # Save all plots\n    plot_roc_curves(roc_results, dataset_name, save_dir)\n    plot_auc_vs_threshold(roc_results, dataset_name, save_dir)\n    plot_event_yields(counts_sig, counts_bkg, dataset_name, save_dir)\n\n    compare_feature_importances(\n        rf_low=rf_models[15],\n        rf_high=rf_models[60],\n        feature_cols=feature_cols,\n        dataset_name=dataset_name,\n        save_dir=save_dir\n    )\n\n    print(f\" Completed. Plots saved in: {save_dir}\")\n","metadata":{"id":"RzOg8A7XG8g1"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Run the analysis for different datasets","metadata":{"id":"7DcXd2rKHWoR"}},{"cell_type":"code","source":"run_full_analysis(\"HAHMggf\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeXEm3Q_G8fF","outputId":"60586c7e-7790-470e-c7c9-17eefc3fd942"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Running full analysis for: HAHMggf ===\n","\n","→ Applying jet pt threshold T = 5 GeV\n","\n","→ Applying jet pt threshold T = 15 GeV\n","\n","→ Applying jet pt threshold T = 25 GeV\n","\n","→ Applying jet pt threshold T = 45 GeV\n","\n","→ Applying jet pt threshold T = 60 GeV\n","\n","→ Applying jet pt threshold T = 80 GeV\n"," Completed. Plots saved in: /content/drive/MyDrive/Datasets/plots_with_OR/HAHMggf\n"]}],"execution_count":null},{"cell_type":"code","source":"run_full_analysis(\"HNLeemu\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_g72scZNIgRf","outputId":"367dc677-b41b-45d6-face-d68c991ac52e"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Running full analysis for: HNLeemu ===\n","\n","→ Applying jet pt threshold T = 5 GeV\n","\n","→ Applying jet pt threshold T = 15 GeV\n","\n","→ Applying jet pt threshold T = 25 GeV\n","\n","→ Applying jet pt threshold T = 45 GeV\n","\n","→ Applying jet pt threshold T = 60 GeV\n","\n","→ Applying jet pt threshold T = 80 GeV\n"," Completed. Plots saved in: /content/drive/MyDrive/Datasets/plots_with_OR/HNLeemu\n"]}],"execution_count":null},{"cell_type":"code","source":"run_full_analysis(\"HtoSUEP\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7r_gOM8VI6Zj","outputId":"ecdd8e9d-b1f3-4448-d9cd-4700975413ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Running full analysis for: HtoSUEP ===\n","\n","→ Applying jet pt threshold T = 5 GeV\n","\n","→ Applying jet pt threshold T = 15 GeV\n","\n","→ Applying jet pt threshold T = 25 GeV\n","\n","→ Applying jet pt threshold T = 45 GeV\n","\n","→ Applying jet pt threshold T = 60 GeV\n","\n","→ Applying jet pt threshold T = 80 GeV\n"," Completed. Plots saved in: /content/drive/MyDrive/Datasets/plots_with_OR/HtoSUEP\n"]}],"execution_count":null},{"cell_type":"code","source":"run_full_analysis(\"VBF_H125_a55a55_4b_ctau1_filtered\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLbhOJKKI6Wr","outputId":"8d417b0e-fdb4-437e-cdcc-7ee0bf329c26"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Running full analysis for: VBF_H125_a55a55_4b_ctau1_filtered ===\n","\n","→ Applying jet pt threshold T = 5 GeV\n","\n","→ Applying jet pt threshold T = 15 GeV\n","\n","→ Applying jet pt threshold T = 25 GeV\n","\n","→ Applying jet pt threshold T = 45 GeV\n","\n","→ Applying jet pt threshold T = 60 GeV\n","\n","→ Applying jet pt threshold T = 80 GeV\n"," Completed. Plots saved in: /content/drive/MyDrive/Datasets/plots_with_OR/VBF_H125_a55a55_4b_ctau1_filtered\n"]}],"execution_count":null},{"cell_type":"code","source":"run_full_analysis(\"Znunu\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5VNBRo-I6T9","outputId":"615dad2b-6da0-4c0f-a885-76a04d40426c"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Running full analysis for: Znunu ===\n","\n","→ Applying jet pt threshold T = 5 GeV\n","\n","→ Applying jet pt threshold T = 15 GeV\n","\n","→ Applying jet pt threshold T = 25 GeV\n","\n","→ Applying jet pt threshold T = 45 GeV\n","\n","→ Applying jet pt threshold T = 60 GeV\n","\n","→ Applying jet pt threshold T = 80 GeV\n"," Completed. Plots saved in: /content/drive/MyDrive/Datasets/plots_with_OR/Znunu\n"]}],"execution_count":null},{"cell_type":"code","source":"run_full_analysis(\"ggF_H125_a16a16_4b_ctau10_filtered\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSUTfOjgI_pf","outputId":"03842afa-315e-42f4-dae6-2d0e8b7322f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Running full analysis for: ggF_H125_a16a16_4b_ctau10_filtered ===\n","\n","→ Applying jet pt threshold T = 5 GeV\n","\n","→ Applying jet pt threshold T = 15 GeV\n","\n","→ Applying jet pt threshold T = 25 GeV\n","\n","→ Applying jet pt threshold T = 45 GeV\n","\n","→ Applying jet pt threshold T = 60 GeV\n","\n","→ Applying jet pt threshold T = 80 GeV\n"," Completed. Plots saved in: /content/drive/MyDrive/Datasets/plots_with_OR/ggF_H125_a16a16_4b_ctau10_filtered\n"]}],"execution_count":null},{"cell_type":"code","source":"run_full_analysis(\"hh_bbbb_vbf_novhh_5fs_l1cvv1cv1\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oBNKI-5I_m7","outputId":"35e1b152-35b2-4a22-e65e-4fe417eede86"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Running full analysis for: hh_bbbb_vbf_novhh_5fs_l1cvv1cv1 ===\n","\n","→ Applying jet pt threshold T = 5 GeV\n","\n","→ Applying jet pt threshold T = 15 GeV\n","\n","→ Applying jet pt threshold T = 25 GeV\n","\n","→ Applying jet pt threshold T = 45 GeV\n","\n","→ Applying jet pt threshold T = 60 GeV\n","\n","→ Applying jet pt threshold T = 80 GeV\n"," Completed. Plots saved in: /content/drive/MyDrive/Datasets/plots_with_OR/hh_bbbb_vbf_novhh_5fs_l1cvv1cv1\n"]}],"execution_count":null},{"cell_type":"code","source":"run_full_analysis(\"all_signals\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aCv25LnzI_ky","outputId":"fa6ec2eb-2637-482c-83b8-4f47a8ecc1cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Running full analysis for: all_signals ===\n","\n","→ Applying jet pt threshold T = 5 GeV\n","\n","→ Applying jet pt threshold T = 15 GeV\n","\n","→ Applying jet pt threshold T = 25 GeV\n","\n","→ Applying jet pt threshold T = 45 GeV\n","\n","→ Applying jet pt threshold T = 60 GeV\n","\n","→ Applying jet pt threshold T = 80 GeV\n"," Completed. Plots saved in: /content/drive/MyDrive/Datasets/plots_with_OR/all_signals\n"]}],"execution_count":null},{"cell_type":"markdown","source":"## GNN: Supervised Classification","metadata":{"id":"i-A1RkiG1JoN"}},{"cell_type":"code","source":"!pip install -q torch_geometric","metadata":{"id":"aboJ-QBw5fad","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d1c8a8ed-13d5-4577-d969-db081346fe7a","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:05.115450Z","iopub.execute_input":"2025-11-29T13:37:05.115733Z","iopub.status.idle":"2025-11-29T13:37:10.289033Z","shell.execute_reply.started":"2025-11-29T13:37:05.115711Z","shell.execute_reply":"2025-11-29T13:37:10.288312Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# imports for GNN\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.utils import to_networkx\nfrom torch_geometric.nn import (\n    GCNConv,\n    SAGEConv,\n    GINConv,\n    global_mean_pool,\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-KZtgEyj4gK2","outputId":"1cbcdcc2-669c-44b2-a2da-247de0a898c8","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:10.290574Z","iopub.execute_input":"2025-11-29T13:37:10.290897Z","iopub.status.idle":"2025-11-29T13:37:21.800449Z","shell.execute_reply.started":"2025-11-29T13:37:10.290842Z","shell.execute_reply":"2025-11-29T13:37:21.799789Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# fixed object ordering: [MET, e, j, mu, ph] for one-hot encoding\nOBJ_TYPES  = [\"MET\", \"e\", \"j\", \"mu\", \"ph\"]\nONEHOT_MAP = {\n    \"MET\": np.array([1, 0, 0, 0, 0], dtype=np.float32),\n    \"e\"  : np.array([0, 1, 0, 0, 0], dtype=np.float32),\n    \"j\"  : np.array([0, 0, 1, 0, 0], dtype=np.float32),\n    \"mu\" : np.array([0, 0, 0, 1, 0], dtype=np.float32),\n    \"ph\" : np.array([0, 0, 0, 0, 1], dtype=np.float32),\n}\n\n\ndef _complete_graph_edge_index(n_nodes: int) -> torch.Tensor:\n    \"\"\"\n    Build a fully-connected, directed edge_index without self-loops.\n    For undirected GNNs, having i->j and j->i is fine.\n    \"\"\"\n    rows = []\n    cols = []\n    for i in range(n_nodes):\n        for j in range(n_nodes):\n            if i == j:\n                continue\n            rows.append(i)\n            cols.append(j)\n    return torch.tensor([rows, cols], dtype=torch.long)\n\n\ndef build_obj_df_and_pyg_dataset(df: pd.DataFrame):\n    \"\"\"\n    Parameters\n    ----------\n    df : pd.DataFrame\n        One of the ML_dict dataframes (e.g. ML_dict['all_signals']).\n        Must have columns:\n        j0pt..j5phi, e0pt..e2phi, mu0pt..mu2phi, ph0pt..ph2phi,\n        METpt, METeta, METphi, target.\n        (run_number, event_number, weight may be present but are ignored.)\n\n    Returns\n    -------\n    obj_df : pd.DataFrame\n        Per-object table with columns:\n        [pT, eta, phi, obj, event, target, obj_MET, obj_e, obj_j, obj_mu, obj_ph]\n\n    data_list : list[torch_geometric.data.Data]\n        One PyG Data object per event (row of df).\n        Each graph has:\n          - x: node features [pT, eta, phi, one-hot(obj-type)]  (shape [N_nodes, 8])\n          - edge_index: complete graph between all nodes\n          - y: event label (0 = EB_test/background, 1 = signal)\n    \"\"\"\n\n    obj_rows = []      # rows for per-object pandas table\n    data_list = []     # PyG graphs\n\n    for event_idx, (_, row) in enumerate(df.iterrows()):\n        node_feats = []\n\n        # ---- event-level label ----\n        tgt_raw = row[\"target\"]\n        if isinstance(tgt_raw, str):\n            # EB_test is background (0), everything else is signal (1)\n            target = 0 if tgt_raw == \"EB_test\" else 1\n        else:\n            target = int(tgt_raw)\n\n        # ---- helper to add one object ----\n        def add_obj(pt, eta, phi, obj_name):\n            # treat (0,0,0) as \"no object\" and skip\n            if pt == 0 and eta == 0 and phi == 0:\n                return\n\n            oh = ONEHOT_MAP[obj_name]  # length-5 one-hot\n\n            # Node features for PyG: [pT, eta, phi, one-hot]\n            node_feats.append(np.concatenate([[pt, eta, phi], oh], axis=0))\n\n            # Row for per-object dataframe\n            obj_rows.append({\n                \"pT\": float(pt),\n                \"eta\": float(eta),\n                \"phi\": float(phi),\n                \"obj\": obj_name,\n                \"event\": event_idx,      # internal event id\n                \"target\": int(target),\n                \"obj_MET\": int(obj_name == \"MET\"),\n                \"obj_e\"  : int(obj_name == \"e\"),\n                \"obj_j\"  : int(obj_name == \"j\"),\n                \"obj_mu\" : int(obj_name == \"mu\"),\n                \"obj_ph\" : int(obj_name == \"ph\"),\n            })\n\n        # ---- jets j0..j5 ----\n        for i in range(6):\n            add_obj(row[f\"j{i}pt\"],  row[f\"j{i}eta\"],  row[f\"j{i}phi\"],  \"j\")\n\n        # ---- electrons e0..e2 ----\n        for i in range(3):\n            add_obj(row[f\"e{i}pt\"],  row[f\"e{i}eta\"],  row[f\"e{i}phi\"],  \"e\")\n\n        # ---- muons mu0..mu2 ----\n        for i in range(3):\n            add_obj(row[f\"mu{i}pt\"], row[f\"mu{i}eta\"], row[f\"mu{i}phi\"], \"mu\")\n\n        # ---- photons ph0..ph2 ----\n        for i in range(3):\n            add_obj(row[f\"ph{i}pt\"], row[f\"ph{i}eta\"], row[f\"ph{i}phi\"], \"ph\")\n\n        # ---- MET (single object) ----\n        add_obj(row[\"METpt\"], row[\"METeta\"], row[\"METphi\"], \"MET\")\n\n        # If the event has no surviving objects, skip graph creation\n        if len(node_feats) == 0:\n            continue\n\n        x = torch.tensor(np.vstack(node_feats), dtype=torch.float32)  # [N_nodes, 8]\n        edge_index = _complete_graph_edge_index(x.size(0))\n        y = torch.tensor([target], dtype=torch.long)                  # [1]\n\n        data = Data(x=x, edge_index=edge_index, y=y)\n        data_list.append(data)\n\n    obj_df = pd.DataFrame(obj_rows)\n    return obj_df, data_list\n","metadata":{"id":"6JwwzJ3AI_ik","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:21.801627Z","iopub.execute_input":"2025-11-29T13:37:21.802157Z","iopub.status.idle":"2025-11-29T13:37:21.815264Z","shell.execute_reply.started":"2025-11-29T13:37:21.802135Z","shell.execute_reply":"2025-11-29T13:37:21.814482Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df_all = ML_dict[\"all_signals\"]\nobj_df, pyg_dataset = build_obj_df_and_pyg_dataset(df_all)","metadata":{"id":"hKE0EgKU4x4M"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(obj_df.head())\nprint(len(pyg_dataset), \"graphs\")\nprint(pyg_dataset[0])   # first PyG Data object","metadata":{"id":"ezVUILN04x11"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: train/val/test split and DataLoaders\n\n# Extract labels to stratify by event-level target\ny_all = np.array([g.y.item() for g in pyg_dataset])\n\nidx_train_val, idx_test = train_test_split(\n    np.arange(len(pyg_dataset)),\n    test_size=0.15,\n    stratify=y_all,\n    random_state=42,\n)\n\ny_train_val = y_all[idx_train_val]\n\nidx_train, idx_val = train_test_split(\n    idx_train_val,\n    test_size=0.1765,  # 0.85 * 0.1765 ≈ 0.15 → 70/15/15 split overall\n    stratify=y_train_val,\n    random_state=42,\n)\n\ntrain_graphs = [pyg_dataset[i] for i in idx_train]\nval_graphs   = [pyg_dataset[i] for i in idx_val]\ntest_graphs  = [pyg_dataset[i] for i in idx_test]\n\nprint(f\"Train: {len(train_graphs)}, Val: {len(val_graphs)}, Test: {len(test_graphs)}\")\n\nbatch_size = 64\n\ntrain_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_graphs,   batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_graphs,  batch_size=batch_size, shuffle=False)\n","metadata":{"id":"I0i-tN9i4xzy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: visualise one example graph\n\nfrom torch_geometric.utils import to_networkx\nimport networkx as nx\nimport matplotlib as mpl\n\n# Safely choose an example event\nexample_idx = 721  # change this to look at other events, must be < len(train_graphs)\nexample_data = train_graphs[example_idx]\n\n# Convert to NetworkX graph (nodes are 0 .. N-1, same order as example_data.x)\nG = to_networkx(example_data, to_undirected=True)\n\n# Deduce object type from the one-hot part of the node features (x[:, 3:])\n# Uses the SAME ordering as in build_obj_df_and_pyg_dataset:\n# OBJ_TYPES = [\"MET\", \"e\", \"j\", \"mu\", \"ph\"]\ntry:\n    OBJ_TYPES  # use the global definition if it exists\nexcept NameError:\n    OBJ_TYPES = [\"MET\", \"e\", \"j\", \"mu\", \"ph\"]\n\nx = example_data.x.cpu().numpy()          # shape [N_nodes, 8]\none_hot = x[:, 3:]                        # shape [N_nodes, 5]\nobj_type_idx = one_hot.argmax(axis=1)     # integers 0..4\n\n# Map each node to a specific color\ncmap = mpl.colormaps.get_cmap(\"tab10\")\nnode_colors = [cmap(i) for i in obj_type_idx]\n\nfig, ax = plt.subplots(figsize=(5, 5))\npos = nx.spring_layout(G, seed=42)\n\nnx.draw(\n    G,\n    pos,\n    node_color=node_colors,\n    with_labels=False,\n    node_size=300,\n    edge_color=\"lightgray\",\n    ax=ax,\n)\n\n# Build a matching legend\nfor i, name in enumerate(OBJ_TYPES):\n    ax.scatter([], [], color=cmap(i), label=name, s=80)\nax.legend(title=\"Object type\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n\nax.set_title(f\"Example event (train index = {example_idx}) as a graph\")\nplt.tight_layout()\nplt.show()\n\n# (optional) print a quick summary of how many objects of each type are in this event\nunique, counts = np.unique(obj_type_idx, return_counts=True)\nprint(\"Object counts in this event:\")\nfor idx, cnt in zip(unique, counts):\n    print(f\"  {OBJ_TYPES[idx]}: {cnt}\")\n","metadata":{"id":"UnISHML0AF5E"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: define a simple GNN for event classification\n\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nimport torch.nn as nn\n\nclass EventGNN(torch.nn.Module):\n    def __init__(self, in_channels=8, hidden_channels=32, num_classes=2):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.lin1  = nn.Linear(hidden_channels, hidden_channels)\n        self.lin2  = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch  # batch is added by DataLoader\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n\n        # global pooling → [num_graphs, hidden_channels]\n        x = global_mean_pool(x, batch)\n\n        x = self.lin1(x)\n        x = F.relu(x)\n        x = self.lin2(x)          # logits for 2 classes\n\n        return x\n\nmodel = EventGNN(in_channels=train_graphs[0].x.size(1), hidden_channels=64, num_classes=2).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\nprint(model)\n","metadata":{"id":"JaJ0v_NVAF2d"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5 (fixed): pictorial representation of the GNN architecture (schematic)\n\nimport matplotlib.patches as patches\n\nlayers = [\n    \"Input\\n(node features)\",\n    \"GCNConv(8 → 64)\",\n    \"ReLU\",\n    \"GCNConv(64 → 64)\",\n    \"ReLU\",\n    \"global_mean_pool\",\n    \"Linear(64 → 64)\",\n    \"ReLU\",\n    \"Linear(64 → 2)\\n(logits)\",\n]\n\nn_layers   = len(layers)\nbox_width  = 1.4\nbox_height = 0.8\ngap        = 0.5\nx0         = 0.5   # left margin\ny0         = 0.2   # bottom margin\n\n# Figure width scales with number of layers so everything is visible\nfig_width = max(12, n_layers * (box_width + gap) * 0.6)\nfig, ax = plt.subplots(figsize=(fig_width+2, 2.8))\nax.axis(\"off\")\n\nfor i, name in enumerate(layers):\n    x_left = x0 + i * (box_width + gap)\n\n    rect = patches.FancyBboxPatch(\n        (x_left, y0),\n        box_width,\n        box_height,\n        boxstyle=\"round,pad=0.15\",\n        edgecolor=\"black\",\n        facecolor=\"lightblue\",\n    )\n    ax.add_patch(rect)\n\n    ax.text(\n        x_left + box_width / 2.0,\n        y0 + box_height / 2.0,\n        name,\n        ha=\"center\",\n        va=\"center\",\n        fontsize=8,\n    )\n\n    # Draw arrows between blocks\n    if i < n_layers - 1:\n        x_start = x_left + box_width\n        x_end   = x0 + (i + 1) * (box_width + gap)\n        ax.annotate(\n            \"\",\n            xy=(x_end, y0 + box_height / 2.0),\n            xytext=(x_start, y0 + box_height / 2.0),\n            arrowprops=dict(arrowstyle=\"->\"),\n        )\n\n# Make sure everything is in view\nx_max = x0 + n_layers * (box_width + gap)\nax.set_xlim(0, x_max + 0.5)\nax.set_ylim(0, y0 + box_height + 0.5)\n\nplt.title(\"Schematic of EventGNN architecture\", pad=20)\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"9RUql6cHAF0W"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Should take appoximately 17 mins to train 20 epochs","metadata":{"id":"uPQZihRyGUWQ"}},{"cell_type":"code","source":"# Cell 6: training & validation loop\n\nfrom torch_geometric.loader import DataLoader\n\nnum_epochs = 10\n\ntrain_losses = []\nval_losses   = []\nval_accuracies = []\n\nfor epoch in range(1, num_epochs + 1):\n    # --- Training ---\n    model.train()\n    total_loss = 0.0\n\n    for batch in train_loader:\n        batch = batch.to(device)\n        optimizer.zero_grad()\n        out = model(batch)                # [batch_size_graphs, 2]\n        loss = criterion(out, batch.y)    # batch.y shape [batch_size_graphs]\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch.num_graphs\n\n    avg_train_loss = total_loss / len(train_graphs)\n    train_losses.append(avg_train_loss)\n\n    # --- Validation ---\n    model.eval()\n    total_val_loss = 0.0\n    correct = 0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = batch.to(device)\n            out = model(batch)\n            loss = criterion(out, batch.y)\n            total_val_loss += loss.item() * batch.num_graphs\n\n            preds = out.argmax(dim=1)\n            correct += (preds == batch.y).sum().item()\n\n    avg_val_loss = total_val_loss / len(val_graphs)\n    val_losses.append(avg_val_loss)\n\n    val_acc = correct / len(val_graphs)\n    val_accuracies.append(val_acc)\n\n    print(f\"Epoch {epoch:02d} | \"\n          f\"Train loss: {avg_train_loss:.4f} | \"\n          f\"Val loss: {avg_val_loss:.4f} | \"\n          f\"Val acc: {val_acc:.3f}\")\n","metadata":{"id":"7bPS-2u2AT9V"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: training & validation loss curves\n\nplt.figure(figsize=(6, 4))\nplt.plot(train_losses, label=\"Train loss\")\nplt.plot(val_losses,   label=\"Val loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training / Validation loss\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"p8v0qKdEAT7A"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: evaluation on test set and ROC curve\n\nmodel.eval()\nall_probs = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = batch.to(device)\n        logits = model(batch)                      # [B, 2]\n        probs = F.softmax(logits, dim=1)[:, 1]     # probability of class 1 (signal)\n        all_probs.append(probs.cpu())\n        all_labels.append(batch.y.cpu())\n\nall_probs = torch.cat(all_probs).numpy()\nall_labels = torch.cat(all_labels).numpy()\n\n# Basic accuracy\npred_labels = (all_probs >= 0.5).astype(int)\ntest_acc = (pred_labels == all_labels).mean()\nprint(f\"Test accuracy: {test_acc:.3f}\")\n\n# ROC curve\nfpr, tpr, thresholds = roc_curve(all_labels, all_probs)\nroc_auc = auc(fpr, tpr)\nprint(f\"AUC: {roc_auc:.3f}\")\n\nplt.figure(figsize=(6, 6))\nplt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\nplt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC curve (test set)\")\nplt.legend(loc=\"lower right\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"wz1ZrQsdAT4t"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GNN: Unsupervised Anomaly Detection (GCN / GraphSAGE / GIN)\n\nIn this section we train a simple Graph Autoencoder (GAE) in an\nunsupervised way on background (EB_test) events and compare three\nconvolution operators in PyTorch Geometric:\n\n- GCNConv\n- SAGEConv (GraphSAGE)\n- GINConv\n\nWe keep the same graph construction (`build_obj_df_and_pyg_dataset`) and\noptionally apply jet $p_T$ cuts. We support:\n\n- **No jet $p_T$ cut** (whole dataset),\n- **One fixed jet $p_T$ cut**, or\n- **A scan over several jet $p_T$ thresholds.**\n\nFor each configuration and convolution, we:\n1. Build PyG graphs.\n2. Train a GAE on background-only graphs.\n3. Compute graph-level reconstruction errors on a mixed test set.\n4. Build ROC curves / AUC for anomaly detection.\n5. Visualise latent space.\n6. Plot input vs reconstructed $(\\eta,\\phi)$ for a few events using  \n   `plot_eta_phi_input_vs_reco_and_save`.\n","metadata":{"id":"FKRE-_MIHxxr"}},{"cell_type":"markdown","source":"Run the cell for `build_obj_df_and_pyg_dataset(df)` in the previous section. Takes $\\sim$ 4 minutes.","metadata":{"id":"JjPJXwuyF1Ce"}},{"cell_type":"code","source":"# Cell 1: build PyG graphs and split background vs signal\n\ndf = ML_dict[\"Znunu\"]\n\n# Uses the previously defined function (EB_test -> 0, signal -> 1)\nobj_df, pyg_list = build_obj_df_and_pyg_dataset(df)\n\nprint(f\"Total events (graphs): {len(pyg_list)}\")\nprint(\"Labels (0=EB_test/background, 1=signal):\",\n      {int(y): int(sum(g.y.item() == y for g in pyg_list)) for y in [0,1]})\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Extract labels\nlabels = np.array([g.y.item() for g in pyg_list])\nbkg_idx    = np.where(labels == 0)[0]   # EB_test\nsignal_idx = np.where(labels == 1)[0]   # everything else\nprint(f\"Background graphs: {len(bkg_idx)}, Signal graphs: {len(signal_idx)}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"btBtmdTMHw8r","outputId":"b1f89980-548e-4f77-e213-f1f16bd6d773"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total events (graphs): 17242\n","Labels (0=EB_test/background, 1=signal): {0: 8017, 1: 9225}\n","Using device: cpu\n","Background graphs: 8017, Signal graphs: 9225\n"]}],"execution_count":null},{"cell_type":"markdown","source":"### All helper functions","metadata":{"id":"TkJNaULUD1hb"}},{"cell_type":"code","source":"# Cell 1: global config for plots & jet pT selection\n\n# ------------------------------------------------------------------\n# Base directory for plots\n# ------------------------------------------------------------------\nif \"PLOT_BASE_DIR\" not in globals():\n    # Adjust if you use a different path\n    PLOT_BASE_DIR = \"/kaggle/working/GAEplots\"\n\nGNN_UNSUP_BASE_DIR = os.path.join(PLOT_BASE_DIR, \"GAE_detailedstudy\")\nos.makedirs(GNN_UNSUP_BASE_DIR, exist_ok=True)\nprint(\"GNN unsupervised results will be saved under:\", GNN_UNSUP_BASE_DIR)\n\n# ------------------------------------------------------------------\n# Jet pT thresholds & selection\n# ------------------------------------------------------------------\ntry:\n    JET_PT_THRESHOLDS\n    JET_PT_COLS\nexcept NameError:\n    # Fallback if RF section hasn't run\n    JET_PT_THRESHOLDS = [5, 15, 25, 45, 60, 80]\n    JET_PT_COLS = [f\"j{i}pt\" for i in range(6)]\n\ndef apply_jet_pt_threshold(df, threshold):\n    \"\"\"\n    Returns a filtered dataframe where all nonzero jets have pt >= threshold.\n    Condition: for each jet jX,\n      keep event if (jXpt == 0) or (jXpt >= threshold)\n    \"\"\"\n    mask = np.ones(len(df), dtype=bool)\n    for col in JET_PT_COLS:\n        if col in df.columns:\n            mask &= (df[col] == 0) | (df[col] >= threshold)\n    return df[mask]\n\n# Object types for η–φ plots (node one-hot order)\nif \"OBJ_TYPES\" not in globals():\n    OBJ_TYPES = [\"MET\", \"e\", \"j\", \"mu\", \"ph\"]\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZmrdCmcKhkic","outputId":"647c70c7-dfd4-4e45-8c2f-c0e997785f2b","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:28.426946Z","iopub.execute_input":"2025-11-29T13:37:28.427680Z","iopub.status.idle":"2025-11-29T13:37:28.434507Z","shell.execute_reply.started":"2025-11-29T13:37:28.427655Z","shell.execute_reply":"2025-11-29T13:37:28.433708Z"}},"outputs":[{"name":"stdout","text":"GNN unsupervised results will be saved under: /kaggle/working/GAEplots/GAE_detailedstudy\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"PLOT_BASE_DIR      =\", PLOT_BASE_DIR)\nprint(\"GNN_UNSUP_BASE_DIR =\", GNN_UNSUP_BASE_DIR)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r21OaUXq9D2V","outputId":"014a3af6-f595-4a02-8536-2fb75dbbc16e","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:29.798088Z","iopub.execute_input":"2025-11-29T13:37:29.798387Z","iopub.status.idle":"2025-11-29T13:37:29.802606Z","shell.execute_reply.started":"2025-11-29T13:37:29.798363Z","shell.execute_reply":"2025-11-29T13:37:29.801767Z"}},"outputs":[{"name":"stdout","text":"PLOT_BASE_DIR      = /kaggle/working/GAEplots\nGNN_UNSUP_BASE_DIR = /kaggle/working/GAEplots/GAE_detailedstudy\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from torch.nn import Sequential, Linear, ReLU\n\nclass GraphAutoEncoder(nn.Module):\n    \"\"\"\n    Node-wise graph autoencoder.\n\n    Encoder:\n      - conv1 (GCN/SAGE/GIN) -> ReLU -> (optional BatchNorm) -> Dropout\n      - conv_mid (GCN/SAGE/GIN, hidden_channels -> hidden_channels/2)\n           -> ReLU -> (optional BatchNorm) -> Dropout\n      - conv2 (GCN/SAGE/GIN, hidden_channels/2 -> latent) -> ReLU -> (optional BatchNorm) -> Dropout\n\n    Decoder:\n      - Linear -> ReLU -> Dropout -> Linear\n\n    Works on generic node features (in_channels), not hard-coded to 8.\n    \"\"\"\n    def __init__(\n        self,\n        in_channels,\n        hidden_channels=64,\n        latent_channels=16,\n        dropout=0.1,\n        conv_type=\"gcn\",\n        sage_aggr=\"mean\",\n        use_batchnorm=False,\n    ):\n        super().__init__()\n\n        self.conv_type = conv_type.lower()\n        self.dropout = dropout\n        self.use_batchnorm = use_batchnorm\n\n        # intermediate hidden size\n        mid_channels = max(1, hidden_channels // 2)\n        self.mid_channels = mid_channels\n\n        # ----- Encoder convolutions -----\n        if self.conv_type == \"gcn\":\n            self.conv1     = GCNConv(in_channels,     hidden_channels)\n            self.conv_mid  = GCNConv(hidden_channels, mid_channels)\n            self.conv2     = GCNConv(mid_channels,    latent_channels)\n\n        elif self.conv_type == \"sage\":\n            self.conv1     = SAGEConv(in_channels,     hidden_channels, aggr=sage_aggr)\n            self.conv_mid  = SAGEConv(hidden_channels, mid_channels,    aggr=sage_aggr)\n            self.conv2     = SAGEConv(mid_channels,    latent_channels, aggr=sage_aggr)\n\n        elif self.conv_type == \"gin\":\n            mlp1 = Sequential(\n                Linear(in_channels, hidden_channels),\n                ReLU(),\n                Linear(hidden_channels, hidden_channels),\n            )\n            mlp_mid = Sequential(\n                Linear(hidden_channels, mid_channels),\n                ReLU(),\n                Linear(mid_channels, mid_channels),\n            )\n            mlp2 = Sequential(\n                Linear(mid_channels, latent_channels),\n                ReLU(),\n                Linear(latent_channels, latent_channels),\n            )\n            self.conv1    = GINConv(mlp1)\n            self.conv_mid = GINConv(mlp_mid)\n            self.conv2    = GINConv(mlp2)\n\n        else:\n            raise ValueError(f\"Unknown conv_type '{conv_type}'. Use 'gcn', 'sage', or 'gin'.\")\n\n        if self.use_batchnorm:\n            self.bn1     = nn.BatchNorm1d(hidden_channels)\n            self.bn_mid  = nn.BatchNorm1d(mid_channels)\n            self.bn2     = nn.BatchNorm1d(latent_channels)\n\n        # ----- Decoder (unchanged) -----\n        self.dec_lin1 = nn.Linear(latent_channels, hidden_channels)\n        self.dec_lin2 = nn.Linear(hidden_channels, in_channels)\n\n    def encode(self, x, edge_index):\n        # First hidden layer\n        h = self.conv1(x, edge_index)\n        if self.use_batchnorm:\n            h = self.bn1(h)\n        h = F.relu(h)\n        h = F.dropout(h, p=self.dropout, training=self.training)\n\n        # New intermediate hidden layer (hidden_channels -> hidden_channels/2)\n        h_mid = self.conv_mid(h, edge_index)\n        if self.use_batchnorm:\n            h_mid = self.bn_mid(h_mid)\n        h_mid = F.relu(h_mid)\n        h_mid = F.dropout(h_mid, p=self.dropout, training=self.training)\n\n        # Latent layer\n        z = self.conv2(h_mid, edge_index)\n        if self.use_batchnorm:\n            z = self.bn2(z)\n        z = F.relu(z)\n        z = F.dropout(z, p=self.dropout, training=self.training)\n        return z\n\n    def decode(self, z):\n        h = self.dec_lin1(z)\n        h = F.relu(h)\n        h = F.dropout(h, p=self.dropout, training=self.training)\n        x_hat = self.dec_lin2(h)\n        return x_hat\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        z = self.encode(x, edge_index)\n        x_hat = self.decode(z)\n        return x_hat, z\n","metadata":{"id":"DLQW3dMluFQq","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:34.687242Z","iopub.execute_input":"2025-11-29T13:37:34.687855Z","iopub.status.idle":"2025-11-29T13:37:34.698501Z","shell.execute_reply.started":"2025-11-29T13:37:34.687833Z","shell.execute_reply":"2025-11-29T13:37:34.697667Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 4: Train/val/test splits, training loop, scoring\n\ndef make_gae_splits_and_loaders(pyg_list, batch_size=128, random_state=42):\n    \"\"\"\n    Splits a list of PyG graphs into train/val/test for unsupervised anomaly detection.\n\n    - Training uses *only* background graphs (y == 0).\n    - Validation and test contain both background and signal.\n\n    Returns\n    -------\n    train_loader, val_loader, test_loader, test_labels, split_info\n    \"\"\"\n    labels = np.array([int(g.y.item()) for g in pyg_list])\n    bkg_idx = np.where(labels == 0)[0]\n    sig_idx = np.where(labels == 1)[0]\n\n    n_total = len(pyg_list)\n    n_bkg = len(bkg_idx)\n    n_sig = len(sig_idx)\n    print(f\"Total graphs={n_total}, background={n_bkg}, signal={n_sig}\")\n\n    if n_bkg < 10 or n_sig < 10:\n        print(\"Not enough background or signal graphs for a meaningful ROC → aborting.\")\n        return None\n\n    from sklearn.model_selection import train_test_split\n\n    # 60/20/20 split of background graphs\n    bkg_train_idx, bkg_tmp_idx = train_test_split(\n        bkg_idx, test_size=0.4, random_state=random_state\n    )\n    bkg_val_idx, bkg_test_idx = train_test_split(\n        bkg_tmp_idx, test_size=0.5, random_state=random_state\n    )\n\n    # 50/50 split of signal graphs into val/test\n    if len(sig_idx) >= 2:\n        sig_val_idx, sig_test_idx = train_test_split(\n            sig_idx, test_size=0.5, random_state=random_state\n        )\n    else:\n        sig_val_idx, sig_test_idx = sig_idx, np.array([], dtype=int)\n\n    train_graphs = [pyg_list[i] for i in bkg_train_idx]\n    val_graphs   = [pyg_list[i] for i in np.concatenate([bkg_val_idx, sig_val_idx])]\n    test_idx     = np.concatenate([bkg_test_idx, sig_test_idx])\n    test_graphs  = [pyg_list[i] for i in test_idx]\n    test_labels  = labels[test_idx]\n\n    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n    val_loader   = DataLoader(val_graphs,   batch_size=batch_size, shuffle=False)\n    test_loader  = DataLoader(test_graphs,  batch_size=batch_size, shuffle=False)\n\n    print(f\"Train bkg graphs: {len(train_graphs)}\")\n    print(f\"Val graphs:       {len(val_graphs)} (bkg={len(bkg_val_idx)}, sig={len(sig_val_idx)})\")\n    print(f\"Test graphs:      {len(test_graphs)} (bkg={len(bkg_test_idx)}, sig={len(sig_test_idx)})\")\n\n    split_info = {\n        \"n_total_graphs\": int(n_total),\n        \"n_bkg_graphs\":   int(n_bkg),\n        \"n_sig_graphs\":   int(n_sig),\n        \"n_train_bkg\":    int(len(bkg_train_idx)),\n        \"n_val_bkg\":      int(len(bkg_val_idx)),\n        \"n_test_bkg\":     int(len(bkg_test_idx)),\n        \"n_val_sig\":      int(len(sig_val_idx)),\n        \"n_test_sig\":     int(len(sig_test_idx)),\n    }\n\n    return train_loader, val_loader, test_loader, test_labels, split_info\n\n\ndef train_gae(\n    model,\n    train_loader,\n    val_loader,\n    device,\n    num_epochs=20,\n    lr=1e-3,\n    weight_decay=1e-5,\n    verbose=False,\n):\n    \"\"\"\n    Trains the GAE with MSE reconstruction loss on node features.\n\n    Returns\n    -------\n    model, history   (history has 'train_loss', 'val_loss')\n    \"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    model.to(device)\n\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(1, num_epochs + 1):\n        # ---- Train ----\n        model.train()\n        total_train_loss = 0.0\n        n_train_batches = 0\n\n        for batch in train_loader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n\n            x_hat, z = model(batch)\n            loss = F.mse_loss(x_hat, batch.x)\n\n            loss.backward()\n            optimizer.step()\n\n            total_train_loss += loss.item()\n            n_train_batches += 1\n\n        avg_train_loss = total_train_loss / max(n_train_batches, 1)\n\n        # ---- Validation ----\n        model.eval()\n        total_val_loss = 0.0\n        n_val_batches = 0\n\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = batch.to(device)\n                x_hat, z = model(batch)\n                loss = F.mse_loss(x_hat, batch.x)\n\n                total_val_loss += loss.item()\n                n_val_batches += 1\n\n        avg_val_loss = total_val_loss / max(n_val_batches, 1)\n\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n\n        if verbose:\n            print(\n                f\"[Epoch {epoch:02d}/{num_epochs}] \"\n                f\"train_loss={avg_train_loss:.4e}, val_loss={avg_val_loss:.4e}\"\n            )\n\n    history = {\n        \"train_loss\": train_losses,\n        \"val_loss\":   val_losses,\n    }\n    return model, history\n\n\ndef compute_gae_scores_and_latent(model, loader, device):\n    \"\"\"\n    Compute graph-level anomaly scores and latent embeddings.\n\n    - score  = mean node-wise reconstruction error per graph\n    - latent = mean-pooled node latent vector per graph\n    - labels = graph labels from batch.y\n    \"\"\"\n    model.eval()\n    all_scores = []\n    all_latent = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in loader:\n            labels = batch.y.cpu().numpy()\n            batch = batch.to(device)\n\n            x_hat, z = model(batch)\n\n            per_elem = F.mse_loss(x_hat, batch.x, reduction=\"none\")   # [N_nodes, F]\n            per_node = per_elem.mean(dim=1)                            # [N_nodes]\n            scores = global_mean_pool(per_node, batch.batch)           # [n_graphs]\n\n            latent = global_mean_pool(z, batch.batch)\n\n            all_scores.append(scores.cpu().numpy())\n            all_latent.append(latent.cpu().numpy())\n            all_labels.append(labels)\n\n    scores = np.concatenate(all_scores, axis=0)\n    latent = np.concatenate(all_latent, axis=0)\n    labels = np.concatenate(all_labels, axis=0)\n\n    return scores, latent, labels\n","metadata":{"id":"0par-eMZ9MIR","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:35.260477Z","iopub.execute_input":"2025-11-29T13:37:35.261198Z","iopub.status.idle":"2025-11-29T13:37:35.275767Z","shell.execute_reply.started":"2025-11-29T13:37:35.261170Z","shell.execute_reply":"2025-11-29T13:37:35.275100Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Cell 5: ROC computation and generic plotting helpers\n\ndef compute_roc_auc(scores, labels, pos_label=1):\n    \"\"\"\n    Compute ROC curve and AUC for anomaly scores.\n    Assumes larger scores = more anomalous (signal-like).\n    \"\"\"\n    fpr, tpr, _ = roc_curve(labels, scores, pos_label=pos_label)\n    roc_auc = auc(fpr, tpr)\n    return fpr, tpr, roc_auc\n\n\ndef plot_gae_loss_curves(history, label_str, out_dir, conv_tag):\n    train_losses = history[\"train_loss\"]\n    val_losses   = history[\"val_loss\"]\n    epochs = np.arange(1, len(train_losses) + 1)\n\n    plt.figure()\n    plt.plot(epochs, train_losses, label=\"train\")\n    plt.plot(epochs, val_losses,   label=\"val\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE loss\")\n    plt.title(f\"GAE loss ({conv_tag}) — {label_str}\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n\n    os.makedirs(out_dir, exist_ok=True)\n    out_path = os.path.join(out_dir, f\"{conv_tag}_loss_{label_str.replace(' ', '_')}.png\")\n    plt.savefig(out_path)\n    plt.close()\n    print(\"Saved loss curve to:\", out_path)\n\n\ndef plot_anomaly_score_distribution(scores, labels, label_str, out_dir, conv_tag):\n    scores = np.asarray(scores)\n    labels = np.asarray(labels)\n\n    bkg_scores = scores[labels == 0]\n    sig_scores = scores[labels == 1]\n\n    lo = np.percentile(scores, 1)\n    hi = np.percentile(scores, 99)\n    if hi <= lo:\n        lo, hi = scores.min(), scores.max()\n    bins = np.linspace(lo, hi, 60)\n\n    plt.figure()\n    plt.hist(bkg_scores, bins=bins, histtype=\"step\", density=True, label=\"bkg\")\n    plt.hist(sig_scores, bins=bins, histtype=\"step\", density=True, label=\"sig\")\n    plt.xlabel(\"Graph-level reconstruction error\")\n    plt.ylabel(\"Density\")\n    plt.title(f\"Anomaly score distribution ({conv_tag}) — {label_str}\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n\n    os.makedirs(out_dir, exist_ok=True)\n    out_path = os.path.join(out_dir, f\"{conv_tag}_score_hist_{label_str.replace(' ', '_')}.png\")\n    plt.savefig(out_path)\n    plt.close()\n    print(\"Saved score histogram to:\", out_path)\n\n\ndef plot_latent_space(latent, labels, label_str, out_dir, conv_tag):\n    \"\"\"\n    Simple 2D latent scatter using PCA.\n    \"\"\"\n    latent = np.asarray(latent)\n    labels = np.asarray(labels)\n\n    if latent.shape[1] < 2:\n        print(\"Latent dimension < 2; skipping latent space plot.\")\n        return\n\n    pca = PCA(n_components=2)\n    z_pca = pca.fit_transform(latent)\n\n    plt.figure()\n    for cls, name, marker in [(0, \"bkg\", \".\"), (1, \"sig\", \"^\")]:\n        mask = labels == cls\n        if mask.sum() == 0:\n            continue\n        plt.scatter(z_pca[mask, 0], z_pca[mask, 1], s=10, alpha=0.6, label=name, marker=marker)\n\n    plt.xlabel(\"PC1\")\n    plt.ylabel(\"PC2\")\n    plt.title(f\"Latent space PCA ({conv_tag}) — {label_str}\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n\n    os.makedirs(out_dir, exist_ok=True)\n    out_path = os.path.join(out_dir, f\"{conv_tag}_latent_{label_str.replace(' ', '_')}.png\")\n    plt.savefig(out_path)\n    plt.close()\n    print(\"Saved latent space plot to:\", out_path)\n\n\ndef plot_roc_curve_single(fpr, tpr, roc_auc, label_str, out_dir, conv_tag):\n    plt.figure()\n    plt.plot(fpr, tpr, label=f\"{conv_tag} (AUC={roc_auc:.3f})\")\n    plt.plot([0, 1], [0, 1], \"k--\", label=\"random\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(f\"ROC — {label_str}\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n\n    os.makedirs(out_dir, exist_ok=True)\n    out_path = os.path.join(out_dir, f\"{conv_tag}_roc_{label_str.replace(' ', '_')}.png\")\n    plt.savefig(out_path)\n    plt.close()\n    print(\"Saved ROC curve to:\", out_path)\n\n\ndef plot_gae_roc_all_thresholds(roc_results, dataset_name, save_dir, conv_tag):\n    \"\"\"\n    For pT scan: overlay ROC curves for multiple thresholds for a single conv_type.\n    roc_results: dict[pt_threshold] = (fpr, tpr, auc)\n    \"\"\"\n    if not roc_results:\n        print(\"No ROC results to plot.\")\n        return\n\n    plt.figure()\n    for T, (fpr, tpr, roc_auc) in sorted(roc_results.items()):\n        plt.plot(fpr, tpr, label=f\"T={T} GeV (AUC={roc_auc:.3f})\")\n\n    plt.plot([0, 1], [0, 1], \"k--\", label=\"random\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(f\"GAE ROC vs jet $p_T$ — {dataset_name} ({conv_tag})\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n\n    os.makedirs(save_dir, exist_ok=True)\n    out_path = os.path.join(save_dir, f\"{conv_tag}_roc_all_thresholds.png\")\n    plt.savefig(out_path)\n    plt.close()\n    print(\"Saved multi-threshold ROC plot to:\", out_path)\n\n\ndef plot_gae_auc_vs_threshold(roc_results, dataset_name, save_dir, conv_tag):\n    \"\"\"\n    AUC vs jet pT threshold for a single conv_type.\n    \"\"\"\n    if not roc_results:\n        print(\"No ROC results to plot.\")\n        return\n\n    thresholds = sorted(roc_results.keys())\n    auc_vals = [roc_results[T][2] for T in thresholds]\n\n    plt.figure()\n    plt.plot(thresholds, auc_vals, marker=\"o\")\n    plt.xlabel(\"Jet $p_T$ threshold [GeV]\")\n    plt.ylabel(\"AUC\")\n    plt.title(f\"GAE AUC vs jet $p_T$ — {dataset_name} ({conv_tag})\")\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n\n    os.makedirs(save_dir, exist_ok=True)\n    out_path = os.path.join(save_dir, f\"{conv_tag}_auc_vs_threshold.png\")\n    plt.savefig(out_path)\n    plt.close()\n    print(\"Saved AUC vs threshold plot to:\", out_path)\n","metadata":{"id":"36_mXUNN9MFq","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:36.056509Z","iopub.execute_input":"2025-11-29T13:37:36.057070Z","iopub.status.idle":"2025-11-29T13:37:36.074813Z","shell.execute_reply.started":"2025-11-29T13:37:36.057049Z","shell.execute_reply":"2025-11-29T13:37:36.073819Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Cell 6 (updated): η–φ visualisation of input vs reconstruction\nimport copy\n\ndef plot_eta_phi_input_vs_reco_and_save(\n    data,\n    model,\n    device,\n    exp_dir,\n    filename_prefix,\n    title_suffix=\"\",\n):\n    \"\"\"\n    Visualise one event as η–φ scatter plots (input vs reconstruction) and save it.\n\n    IMPORTANT: we clone `data` before moving to device to avoid mutating the\n    original dataset graphs (which would cause CPU/GPU mixing inside DataLoader).\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        # Make a deep copy so we do NOT modify the original graph in test_graphs\n        batch = copy.deepcopy(data).to(device)\n        x_hat, z = model(batch)\n        x_hat = x_hat.cpu().numpy()\n\n    # Use the original CPU graph for inputs\n    x_in = data.x.cpu().numpy()\n\n    # Input features\n    pt_in  = x_in[:, 0]\n    eta_in = x_in[:, 1]\n    phi_in = x_in[:, 2]\n\n    # Reconstructed features\n    pt_out  = x_hat[:, 0]\n    eta_out = x_hat[:, 1]\n    phi_out = x_hat[:, 2]\n\n    # Object type from one-hot\n    try:\n        OBJ_TYPES  # noqa: F823\n    except NameError:\n        OBJ_TYPES = [\"MET\", \"e\", \"j\", \"mu\", \"ph\"]\n\n    onehot_in = x_in[:, 3:]\n    type_idx  = onehot_in.argmax(axis=1)\n\n    cmap = mpl.colormaps.get_cmap(\"tab10\")\n\n    def pt_to_size(pt):\n        pt = np.clip(pt, 0, None)\n        if pt.max() <= 0:\n            return np.full_like(pt, 30.0)\n        pt_norm = pt / (pt.max() + 1e-8)\n        return 20.0 + 80.0 * pt_norm\n\n    sizes_in  = pt_to_size(pt_in)\n    sizes_out = pt_to_size(pt_out)\n\n    eta_min = min(eta_in.min(), eta_out.min())\n    eta_max = max(eta_in.max(), eta_out.max())\n    phi_min = min(phi_in.min(), phi_out.min())\n    phi_max = max(phi_in.max(), phi_out.max())\n\n    node_colors = [cmap(int(i)) for i in type_idx]\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 4), constrained_layout=True)\n\n    # ---- Left: input ----\n    ax = axes[0]\n    ax.scatter(\n        eta_in,\n        phi_in,\n        s=sizes_in,\n        c=node_colors,\n        alpha=0.8,\n        edgecolors=\"k\",\n    )\n    ax.set_xlabel(r\"$\\eta$\")\n    ax.set_ylabel(r\"$\\phi$\")\n    ax.set_title(\"Input features \" + title_suffix)\n    ax.set_xlim(eta_min, eta_max)\n    ax.set_ylim(phi_min, phi_max)\n    ax.grid(True, alpha=0.3)\n\n    # ---- Right: reconstruction ----\n    ax = axes[1]\n    ax.scatter(\n        eta_out,\n        phi_out,\n        s=sizes_out,\n        c=node_colors,\n        alpha=0.8,\n        edgecolors=\"k\",\n    )\n    ax.set_xlabel(r\"$\\eta$\")\n    ax.set_ylabel(r\"$\\phi$\")\n    ax.set_title(\"Reconstructed features \" + title_suffix)\n    ax.set_xlim(eta_min, eta_max)\n    ax.set_ylim(phi_min, phi_max)\n    ax.grid(True, alpha=0.3)\n\n    # Legend for object types\n    handles = []\n    labels  = []\n    for i, name in enumerate(OBJ_TYPES):\n        handles.append(\n            plt.Line2D(\n                [], [],\n                marker=\"o\",\n                linestyle=\"\",\n                markersize=8,\n                markerfacecolor=cmap(i),\n                markeredgecolor=\"k\",\n            )\n        )\n        labels.append(name)\n\n    fig.legend(\n        handles,\n        labels,\n        title=\"Object type\",\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, 0.02),\n        ncol=len(OBJ_TYPES),\n    )\n\n    os.makedirs(exp_dir, exist_ok=True)\n    save_path = os.path.join(exp_dir, f\"{filename_prefix}.png\")\n    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n    # plt.show()\n    plt.close(fig)\n    print(f\"Saved {filename_prefix} plot to: {save_path}\")\n\n\ndef plot_eta_phi_examples(\n    model,\n    device,\n    test_graphs,\n    test_labels,\n    exp_dir,\n    max_bkg=3,\n    max_sig=3,\n):\n    \"\"\"\n    Pick up to `max_bkg` background and `max_sig` signal graphs from the test set\n    and make η–φ plots for each.\n\n    Note: Uses the updated `plot_eta_phi_input_vs_reco_and_save` which clones graphs.\n    \"\"\"\n    test_labels = np.array(test_labels)\n\n    bkg_indices = np.where(test_labels == 0)[0]\n    sig_indices = np.where(test_labels == 1)[0]\n\n    print(f\"Found {len(bkg_indices)} background and {len(sig_indices)} signal test events.\")\n\n    n_bkg_to_show = min(max_bkg, len(bkg_indices))\n    n_sig_to_show = min(max_sig, len(sig_indices))\n\n    print(f\"Showing {n_bkg_to_show} background and {n_sig_to_show} signal events.\")\n\n    for k in range(n_bkg_to_show):\n        idx = bkg_indices[k]\n        print(f\"\\nBackground example {k+1} (test index = {idx})\")\n        example_bkg = test_graphs[idx]\n        plot_eta_phi_input_vs_reco_and_save(\n            example_bkg,\n            model,\n            device,\n            exp_dir,\n            filename_prefix=f\"bkg_example_{k+1}_eta_phi\",\n            title_suffix=\"(Background)\",\n        )\n\n    for k in range(n_sig_to_show):\n        idx = sig_indices[k]\n        print(f\"\\nSignal example {k+1} (test index = {idx})\")\n        example_sig = test_graphs[idx]\n        plot_eta_phi_input_vs_reco_and_save(\n            example_sig,\n            model,\n            device,\n            exp_dir,\n            filename_prefix=f\"sig_example_{k+1}_eta_phi\",\n            title_suffix=\"(Signal)\",\n        )\n","metadata":{"id":"CCT9pQAb9MCa","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:37.104595Z","iopub.execute_input":"2025-11-29T13:37:37.104902Z","iopub.status.idle":"2025-11-29T13:37:37.119269Z","shell.execute_reply.started":"2025-11-29T13:37:37.104856Z","shell.execute_reply":"2025-11-29T13:37:37.118550Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Cell 7: Core runner on a PyG graph list (single dataset + selection + conv)\n\ndef run_gae_on_graph_list(\n    pyg_list,\n    dataset_name,\n    selection_label,\n    conv_type=\"gcn\",\n    num_epochs=20,\n    batch_size=128,\n    hidden_channels=64,\n    latent_channels=16,\n    lr=1e-3,\n    weight_decay=1e-5,\n    dropout=0.1,\n    random_state=42,\n    sage_aggr=\"mean\",\n    use_batchnorm=False,\n    n_bkg_examples=3,\n    n_sig_examples=3,\n):\n    \"\"\"\n    Core routine:\n      - Takes list of PyG graphs (with .x, .edge_index, .y).\n      - Splits into train/val/test (background-only training).\n      - Builds GraphAutoEncoder with specified conv_type and hyperparams.\n      - Trains, computes scores & ROC, produces plots.\n\n    Returns a dict containing:\n      - 'summary' (with roc_auc, counts, etc.)\n      - 'fpr', 'tpr', 'scores', 'latent', 'test_labels'\n      - 'model', 'history', 'selection_dir', 'conv_tag', 'sel_label_str'\n    \"\"\"\n    assert len(pyg_list) > 0, \"pyg_list is empty.\"\n\n    conv_type = conv_type.lower()\n    conv_tag = conv_type.upper()\n\n    labels = np.array([int(g.y.item()) for g in pyg_list])\n    n_total = len(pyg_list)\n    n_bkg = int((labels == 0).sum())\n    n_sig = int((labels == 1).sum())\n    print(f\"[{dataset_name}, {selection_label}, {conv_tag}] Graphs total={n_total}, background={n_bkg}, signal={n_sig}\")\n\n    # --- Splits ---\n    splits = make_gae_splits_and_loaders(\n        pyg_list,\n        batch_size=batch_size,\n        random_state=random_state,\n    )\n    if splits is None:\n        return None\n\n    train_loader, val_loader, test_loader, test_labels, split_info = splits\n    test_graphs = list(test_loader.dataset)  # underlying list of graphs\n\n    # --- Model ---\n    in_channels = pyg_list[0].x.size(1)\n    print(\"Inferred in_channels:\", in_channels)\n\n    model = GraphAutoEncoder(\n        in_channels=in_channels,\n        hidden_channels=hidden_channels,\n        latent_channels=latent_channels,\n        dropout=dropout,\n        conv_type=conv_type,\n        sage_aggr=sage_aggr,\n        use_batchnorm=use_batchnorm,\n    ).to(device)\n\n    # --- Train ---\n    model, history = train_gae(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        device=device,\n        num_epochs=num_epochs,\n        lr=lr,\n        weight_decay=weight_decay,\n        verbose=True,\n    )\n\n    # --- Scores / latent on test ---\n    scores, latent, test_labels = compute_gae_scores_and_latent(\n        model, test_loader, device\n    )\n\n    # --- ROC ---\n    fpr, tpr, roc_auc = compute_roc_auc(scores, test_labels, pos_label=1)\n    print(f\"[{dataset_name}, {selection_label}, {conv_tag}] ROC AUC = {roc_auc:.3f}\")\n\n    # --- Directories & label strings ---\n    sel_label_str = f\"{dataset_name}, {selection_label}\"\n    selection_dir = os.path.join(\n        GNN_UNSUP_BASE_DIR,\n        f\"{dataset_name}__{conv_tag}\",\n        selection_label,\n    )\n    os.makedirs(selection_dir, exist_ok=True)\n\n    # --- Plots ---\n    plot_gae_loss_curves(history, sel_label_str, selection_dir, conv_tag)\n    plot_anomaly_score_distribution(scores, test_labels, sel_label_str, selection_dir, conv_tag)\n    plot_latent_space(latent, test_labels, sel_label_str, selection_dir, conv_tag)\n    plot_roc_curve_single(fpr, tpr, roc_auc, sel_label_str, selection_dir, conv_tag)\n\n    # η–φ examples (up to a few background + signal events)\n    try:\n        plot_eta_phi_examples(\n            model,\n            device,\n            test_graphs,\n            test_labels,\n            selection_dir,\n            max_bkg=n_bkg_examples,\n            max_sig=n_sig_examples,\n        )\n    except Exception as e:\n        print(\"Warning: η–φ plotting failed with error:\", e)\n\n    # --- Summary dict ---\n    summary = {\n        \"dataset_name\": dataset_name,\n        \"selection_label\": selection_label,\n        \"conv_type\": conv_type,\n        \"roc_auc\": float(roc_auc),\n        **split_info,\n    }\n\n    return {\n        \"model\": model,\n        \"history\": history,\n        \"scores\": scores,\n        \"latent\": latent,\n        \"test_labels\": test_labels,\n        \"fpr\": fpr,\n        \"tpr\": tpr,\n        \"roc_auc\": roc_auc,\n        \"summary\": summary,\n        \"selection_dir\": selection_dir,\n        \"conv_tag\": conv_tag,\n        \"sel_label_str\": sel_label_str,\n    }\n","metadata":{"id":"IGaUxW2r-XmY","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:38.419484Z","iopub.execute_input":"2025-11-29T13:37:38.419752Z","iopub.status.idle":"2025-11-29T13:37:38.430194Z","shell.execute_reply.started":"2025-11-29T13:37:38.419733Z","shell.execute_reply":"2025-11-29T13:37:38.429538Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Cell 8: Single-dataset driver with optional jet pT threshold\n\ndef run_gae_unsupervised_single(\n    dataset_name=\"Znunu\",\n    conv_type=\"gcn\",\n    pt_threshold=None,\n    num_epochs=20,\n    batch_size=128,\n    hidden_channels=64,\n    latent_channels=16,\n    lr=1e-3,\n    weight_decay=1e-5,\n    dropout=0.1,\n    random_state=42,\n    sage_aggr=\"mean\",\n    use_batchnorm=False,\n    max_events=None,\n):\n    \"\"\"\n    Run a single GAE unsupervised anomaly detection experiment for one dataset\n    and one convolution type, with an optional jet pT threshold.\n\n    - If pt_threshold is None: uses the full dataset.\n    - If pt_threshold is a float/int: events are filtered with apply_jet_pt_threshold.\n    - Training is background-only; evaluation uses held-out background + signal.\n\n    Returns\n    -------\n    dict from run_gae_on_graph_list, or None if there are not enough events/graphs.\n    \"\"\"\n    assert \"ML_dict\" in globals(), \"ML_dict not found; please run the cell that loads the pickle.\"\n    assert \"build_obj_df_and_pyg_dataset\" in globals(), \"build_obj_df_and_pyg_dataset not defined; run that cell first.\"\n\n    df = ML_dict[dataset_name].copy()\n    print(\"\\n\" + \"#\" * 80)\n    print(f\"GAE unsupervised run on dataset: {dataset_name} (conv={conv_type})\")\n\n    if pt_threshold is not None:\n        selection_label = f\"T{pt_threshold}GeV\"\n        df = apply_jet_pt_threshold(df, pt_threshold)\n        print(f\"Applying jet pT threshold: {pt_threshold} GeV → events surviving: {len(df)}\")\n    else:\n        selection_label = \"noPtCut\"\n        print(f\"No jet pT threshold applied → events: {len(df)}\")\n\n    if len(df) == 0:\n        print(\"No events left after selection; skipping.\")\n        return None\n\n    if max_events is not None and len(df) > max_events:\n        df = df.iloc[:max_events].copy()\n        print(f\"Downsampling to first {max_events} events.\")\n\n    # Build PyG dataset\n    obj_df, pyg_list = build_obj_df_and_pyg_dataset(df)\n\n    if len(pyg_list) < 20:\n        print(\"Not enough graphs to train/evaluate; skipping.\")\n        return None\n\n    result = run_gae_on_graph_list(\n        pyg_list=pyg_list,\n        dataset_name=dataset_name,\n        selection_label=selection_label,\n        conv_type=conv_type,\n        num_epochs=num_epochs,\n        batch_size=batch_size,\n        hidden_channels=hidden_channels,\n        latent_channels=latent_channels,\n        lr=lr,\n        weight_decay=weight_decay,\n        dropout=dropout,\n        random_state=random_state,\n        sage_aggr=sage_aggr,\n        use_batchnorm=use_batchnorm,\n    )\n\n    print(\"#\" * 80 + \"\\n\")\n    return result\n","metadata":{"id":"IdkE0pXQ-XiS","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:39.233059Z","iopub.execute_input":"2025-11-29T13:37:39.233681Z","iopub.status.idle":"2025-11-29T13:37:39.241287Z","shell.execute_reply.started":"2025-11-29T13:37:39.233656Z","shell.execute_reply":"2025-11-29T13:37:39.240576Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Cell 9: Compare GCN / GraphSAGE / GIN at a single jet pT selection\n\ndef run_gae_compare_convs_single(\n    dataset_name=\"Znunu\",\n    conv_types=(\"gcn\", \"sage\", \"gin\"),\n    pt_threshold=None,\n    num_epochs=20,\n    batch_size=128,\n    hidden_channels=64,\n    latent_channels=16,\n    lr=1e-3,\n    weight_decay=1e-5,\n    dropout=0.1,\n    random_state=42,\n    sage_aggr=\"mean\",\n    use_batchnorm=False,\n    max_events=None,\n):\n    \"\"\"\n    For a fixed dataset and an optional jet pT selection, run a GAE with multiple\n    convolution types and compare ROC AUCs.\n\n    Returns\n    -------\n    results: dict[conv_type] -> dict (output of run_gae_unsupervised_single)\n    \"\"\"\n    results = {}\n\n    for conv_type in conv_types:\n        print(\"\\n\" + \"=\" * 80)\n        print(f\"Running conv_type = {conv_type}\")\n        print(\"=\" * 80)\n\n        res = run_gae_unsupervised_single(\n            dataset_name=dataset_name,\n            conv_type=conv_type,\n            pt_threshold=pt_threshold,\n            num_epochs=num_epochs,\n            batch_size=batch_size,\n            hidden_channels=hidden_channels,\n            latent_channels=latent_channels,\n            lr=lr,\n            weight_decay=weight_decay,\n            dropout=dropout,\n            random_state=random_state,\n            sage_aggr=sage_aggr,\n            use_batchnorm=use_batchnorm,\n            max_events=max_events,\n        )\n        results[conv_type] = res\n\n    # Quick summary table\n    print(\"\\nSummary of ROC AUCs:\")\n    for conv_type in conv_types:\n        res = results.get(conv_type)\n        if res is None:\n            print(f\"  {conv_type:>4s}:  (no result)\")\n        else:\n            auc_val = res[\"roc_auc\"]\n            print(f\"  {conv_type:>4s}:  AUC = {auc_val:.3f}\")\n\n    return results\n","metadata":{"id":"pQjmtVE7-nQc","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:40.026937Z","iopub.execute_input":"2025-11-29T13:37:40.027187Z","iopub.status.idle":"2025-11-29T13:37:40.033366Z","shell.execute_reply.started":"2025-11-29T13:37:40.027172Z","shell.execute_reply":"2025-11-29T13:37:40.032743Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Cell 10: Full jet pT scan for multiple convolutions\n\ndef run_gae_compare_convs_pt_scan(\n    dataset_name=\"Znunu\",\n    conv_types=(\"gcn\", \"sage\", \"gin\"),\n    pt_thresholds=None,\n    num_epochs=20,\n    batch_size=128,\n    hidden_channels=64,\n    latent_channels=16,\n    lr=1e-3,\n    weight_decay=1e-5,\n    dropout=0.1,\n    random_state=42,\n    sage_aggr=\"mean\",\n    use_batchnorm=False,\n    max_events=None,\n):\n    \"\"\"\n    For a given dataset, scan over a list of jet pT thresholds and for each\n    threshold train/evaluate a GAE with each convolution type.\n\n    Returns\n    -------\n    all_results: dict[conv_type][threshold] -> result dict from run_gae_on_graph_list\n    \"\"\"\n    if pt_thresholds is None:\n        pt_thresholds = JET_PT_THRESHOLDS\n\n    assert \"ML_dict\" in globals(), \"ML_dict not found; please run the cell that loads the pickle.\"\n    assert \"build_obj_df_and_pyg_dataset\" in globals(), \"build_obj_df_and_pyg_dataset not defined; run that cell first.\"\n\n    df_full = ML_dict[dataset_name].copy()\n    print(\"\\n\" + \"#\" * 80)\n    print(f\"Full pT scan for dataset: {dataset_name}\")\n    print(f\"Total events before any jet-pt selection: {len(df_full)}\")\n\n    all_results = {conv: {} for conv in conv_types}\n\n    for T in pt_thresholds:\n        print(\"\\n\" + \"-\" * 80)\n        print(f\"Jet pT threshold T = {T} GeV\")\n        print(\"-\" * 80)\n\n        df_T = apply_jet_pt_threshold(df_full.copy(), T)\n        print(f\"Events surviving after jet pT cut: {len(df_T)}\")\n\n        if len(df_T) == 0:\n            print(\"No events survive; skipping this threshold.\")\n            continue\n\n        if max_events is not None and len(df_T) > max_events:\n            df_T = df_T.iloc[:max_events].copy()\n            print(f\"Downsampling to first {max_events} events.\")\n\n        obj_df_T, pyg_list_T = build_obj_df_and_pyg_dataset(df_T)\n        labels_T = np.array([g.y.item() for g in pyg_list_T])\n\n        n_graphs = len(pyg_list_T)\n        n_bkg = int((labels_T == 0).sum())\n        n_sig = int((labels_T == 1).sum())\n        print(f\"[T={T}] Graphs total={n_graphs}, background={n_bkg}, signal={n_sig}\")\n\n        if n_bkg < 20 or n_sig < 20:\n            print(f\"[T={T}] Not enough background or signal graphs; skipping this threshold.\")\n            continue\n\n        # For each convolution type, reuse the same pyg_list_T\n        for conv_type in conv_types:\n            selection_label = f\"T{T}GeV\"\n            print(f\"\\n>>> Threshold T={T} GeV, conv_type={conv_type}\")\n\n            res = run_gae_on_graph_list(\n                pyg_list=pyg_list_T,\n                dataset_name=dataset_name,\n                selection_label=selection_label,\n                conv_type=conv_type,\n                num_epochs=num_epochs,\n                batch_size=batch_size,\n                hidden_channels=hidden_channels,\n                latent_channels=latent_channels,\n                lr=lr,\n                weight_decay=weight_decay,\n                dropout=dropout,\n                random_state=random_state,\n                sage_aggr=sage_aggr,\n                use_batchnorm=use_batchnorm,\n            )\n            all_results[conv_type][T] = res\n\n    # After the scan, make AUC-vs-threshold and multi-ROC plots per convolution\n    for conv_type in conv_types:\n        conv_results = {\n            T: (res[\"fpr\"], res[\"tpr\"], res[\"roc_auc\"])\n            for T, res in all_results[conv_type].items()\n            if res is not None\n        }\n        if not conv_results:\n            continue\n\n        conv_tag = conv_type.upper()\n        save_dir = os.path.join(\n            GNN_UNSUP_BASE_DIR,\n            f\"{dataset_name}__{conv_tag}\",\n            \"pt_scan\",\n        )\n\n        plot_gae_roc_all_thresholds(conv_results, dataset_name, save_dir, conv_tag)\n        plot_gae_auc_vs_threshold(conv_results, dataset_name, save_dir, conv_tag)\n\n        # Print best threshold for this conv\n        best_T = max(conv_results.keys(), key=lambda T: conv_results[T][2])\n        best_auc = conv_results[best_T][2]\n        print(\n            f\"\\nBest jet pT threshold for conv={conv_type} on {dataset_name}: \"\n            f\"T={best_T} GeV with AUC={best_auc:.3f}\"\n        )\n\n    print(\"\\n\" + \"#\" * 80 + \"\\nFinished full pT scan.\\n\" + \"#\" * 80)\n    return all_results\n","metadata":{"id":"ieTs6L0w-nNO","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:40.871828Z","iopub.execute_input":"2025-11-29T13:37:40.872668Z","iopub.status.idle":"2025-11-29T13:37:40.883400Z","shell.execute_reply.started":"2025-11-29T13:37:40.872633Z","shell.execute_reply":"2025-11-29T13:37:40.882803Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Hyperparameter scan\n\nTakes approximately 1.5 hrs","metadata":{"id":"P-3GS8-XECz9"}},{"cell_type":"code","source":"# Cell X : Hyperparameter scan on a subset of \"all_signals\"\n#                   and show 3 best configs per conv_type\n\n# --------------------------------------------------------------------------------------\n# 1. Build or reuse a PyG graph subset for the \"all_signals\" dataset\n# --------------------------------------------------------------------------------------\n\nMAX_SUBSET_GRAPHS = 2000   # small subset for quick scans; change if you want\nRANDOM_STATE = 42\n\nif \"pyg_list_all_signals_subset\" in globals():\n    # Reuse existing subset to avoid recomputing graphs\n    pyg_subset = pyg_list_all_signals_subset\n    print(f\"Reusing existing pyg_list_all_signals_subset with {len(pyg_subset)} graphs.\")\nelse:\n    # Build graphs from the \"all_signals\" dataframe\n    if \"ML_dict\" not in globals():\n        raise RuntimeError(\"ML_dict is not defined. Make sure you ran the data-loading cells.\")\n\n    print(\"Building PyG graphs for 'all_signals'...\")\n    df_all = ML_dict[\"all_signals\"]\n    obj_df_all, pyg_list_all_signals = build_obj_df_and_pyg_dataset(df_all)\n    print(f\"Total graphs in 'all_signals': {len(pyg_list_all_signals)}\")\n\n    rng = np.random.default_rng(RANDOM_STATE)\n    subset_size = min(MAX_SUBSET_GRAPHS, len(pyg_list_all_signals))\n    subset_indices = rng.choice(len(pyg_list_all_signals), size=subset_size, replace=False)\n    pyg_subset = [pyg_list_all_signals[i] for i in subset_indices]\n\n    # Cache for later cells, so you don't rebuild\n    pyg_list_all_signals_subset = pyg_subset\n    print(f\"Using subset of {subset_size} graphs for hyperparameter search.\")\n\n# --------------------------------------------------------------------------------------\n# 2. Define hyperparameter grid for the scan (includes lr_list)\n# --------------------------------------------------------------------------------------\n\nconv_types       = [\"gcn\", \"sage\", \"gin\"]\nhidden_list      = [32, 64, 128]\nlatent_list      = [4, 8, 16]\ndropout_list     = [0.0, 0.1, 0.2]\nbatchnorm_list   = [False, True]\nsage_aggr_list   = [\"mean\", \"max\", \"sum\"]   # only used for SAGE\nlr_list          = [1e-4, 5e-4, 1e-3]   # learning rates to scan\n\nNUM_EPOCHS   = 10\nBATCH_SIZE   = 100\nWEIGHT_DECAY = 1e-5\n\nsearch_results = []\nconfig_id = 0\n\nprint(\"\\nStarting hyperparameter scan on 'all_signals' subset...\\n\")\n\n# Add imports for stdout redirection\nimport contextlib\nimport io\n\n# Calculate total number of configurations for progress tracking\ntotal_configs = 0\nfor conv_type in conv_types:\n    num_aggrs = len(sage_aggr_list) if conv_type == \"sage\" else 1\n    total_configs += len(hidden_list) * len(latent_list) * len(dropout_list) * len(batchnorm_list) * len(lr_list) * num_aggrs\n\n# --------------------------------------------------------------------------------------\n# 3. Run the scan using your existing run_gae_on_graph_list\n# --------------------------------------------------------------------------------------\n\nfor conv_type in conv_types:\n    for hidden_channels, latent_channels, dropout, use_batchnorm, lr in itertools.product(\n        hidden_list, latent_list, dropout_list, batchnorm_list, lr_list\n    ):\n        # For SAGE we scan over aggregators; for GCN/GIN we just fix \"mean\"\n        aggr_list = sage_aggr_list if conv_type == \"sage\" else [\"mean\"]\n\n        for sage_aggr in aggr_list:\n            config_id += 1\n\n            cfg = dict(\n                conv_type=conv_type,\n                hidden_channels=hidden_channels,\n                latent_channels=latent_channels,\n                dropout=dropout,\n                use_batchnorm=use_batchnorm,\n                sage_aggr=sage_aggr,\n                lr=lr,\n                num_epochs=NUM_EPOCHS,\n                batch_size=BATCH_SIZE,\n                weight_decay=WEIGHT_DECAY,\n            )\n\n            # Use stdout redirection to suppress output from run_gae_on_graph_list\n            f = io.StringIO() # Create a new StringIO object for each config\n            with contextlib.redirect_stdout(f): # Redirect stdout within this context\n                try:\n                    result = run_gae_on_graph_list(\n                        pyg_list=pyg_subset,\n                        dataset_name=\"all_signals\",\n                        selection_label=f\"subset_{len(pyg_subset)}\",\n                        conv_type=conv_type,\n                        num_epochs=NUM_EPOCHS,\n                        batch_size=BATCH_SIZE,\n                        hidden_channels=hidden_channels,\n                        latent_channels=latent_channels,\n                        lr=lr,\n                        weight_decay=WEIGHT_DECAY,\n                        dropout=dropout,\n                        random_state=RANDOM_STATE,\n                        sage_aggr=sage_aggr,\n                        use_batchnorm=use_batchnorm,\n                        # Avoid generating tons of \\u03b7\\u2013\\u03c6 plots during the scan\n                        n_bkg_examples=0,\n                        n_sig_examples=0,\n                    )\n                except Exception as e:\n                    # Print failures explicitly to the main stdout\n                    print(f\"--> Config {config_id} FAILED with error: {e}\")\n                    continue\n\n            # Print a concise summary for each config after it runs\n            # roc_auc = float(result.get(\"roc_auc\", np.nan))\n            # print(f\"[{config_id:3d}/{total_configs:3d}] \"\n            #       f\"Conv: {conv_type:<4s}, H: {hidden_channels:<3d}, L: {latent_channels:<2d}, D: {dropout:<3.1f}, BN: {str(use_batchnorm):<5s}, LR: {lr:<6.0e}, AUC: {roc_auc:.4f}\")\n\n            entry = dict(cfg)\n            entry[\"roc_auc\"] = float(result.get(\"roc_auc\", np.nan))\n            search_results.append(entry)\n\n# --------------------------------------------------------------------------------------\n# 4. Collect and print the best hyperparameters\n# --------------------------------------------------------------------------------------\n\nif not search_results:\n    print(\"\\nNo successful configurations in the scan.\")\nelse:\n    hp_df = pd.DataFrame(search_results)\n\n    # 3 best per conv_type\n    best_hparams_all_signals_subset_by_conv = {}\n\n    print(\"\\n================ Best 3 configurations per conv_type ================\\n\")\n    for conv in conv_types:\n        sub = hp_df[hp_df[\"conv_type\"] == conv]\n        if sub.empty:\n            print(f\"\\nNo successful configs for conv_type = {conv}\")\n            continue\n\n        sub_sorted = sub.sort_values(\"roc_auc\", ascending=False).reset_index(drop=True)\n        top_k = sub_sorted.head(3)\n\n        print(f\"\\n--- {conv.upper()} ---\")\n        print(top_k)\n\n        # Store the single best config for this conv_type\n        best_hparams_all_signals_subset_by_conv[conv] = top_k.iloc[0].to_dict()\n\n    # Overall best configuration (across all conv_types)\n    hp_df_sorted_global = hp_df.sort_values(\"roc_auc\", ascending=False).reset_index(drop=True)\n    best_global = hp_df_sorted_global.iloc[0]\n\n    print(\"\\n================ Best configuration overall (all conv_types) ================\\n\")\n    for col in hp_df_sorted_global.columns:\n        print(f\"{col}: {best_global[col]}\")\n\n    # Also store global-best config if you want to reuse it\n    best_hparams_all_signals_subset = best_global.to_dict()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6qOXfnd_M4c","outputId":"01d40111-7394-44c0-a1b4-3ec51a28b91b","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:37:45.624474Z","iopub.execute_input":"2025-11-29T13:37:45.624724Z","iopub.status.idle":"2025-11-29T14:06:11.160337Z","shell.execute_reply.started":"2025-11-29T13:37:45.624705Z","shell.execute_reply":"2025-11-29T14:06:11.159534Z"}},"outputs":[{"name":"stdout","text":"Building PyG graphs for 'all_signals'...\nTotal graphs in 'all_signals': 604611\nUsing subset of 2000 graphs for hyperparameter search.\n\nStarting hyperparameter scan on 'all_signals' subset...\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_scatter.py:91: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n","output_type":"stream"},{"name":"stdout","text":"\n================ Best 3 configurations per conv_type ================\n\n\n--- GCN ---\n  conv_type  hidden_channels  latent_channels  dropout  use_batchnorm  \\\n0       gcn               64               16      0.1          False   \n1       gcn               64               16      0.0          False   \n2       gcn               64                8      0.1          False   \n\n  sage_aggr      lr  num_epochs  batch_size  weight_decay   roc_auc  \n0      mean  0.0005          10         100       0.00001  0.905542  \n1      mean  0.0005          10         100       0.00001  0.904822  \n2      mean  0.0010          10         100       0.00001  0.904502  \n\n--- SAGE ---\n  conv_type  hidden_channels  latent_channels  dropout  use_batchnorm  \\\n0      sage              128               16      0.1          False   \n1      sage               32                4      0.0          False   \n2      sage              128               16      0.0          False   \n\n  sage_aggr      lr  num_epochs  batch_size  weight_decay   roc_auc  \n0       sum  0.0010          10         100       0.00001  0.911313  \n1       sum  0.0001          10         100       0.00001  0.907302  \n2       sum  0.0010          10         100       0.00001  0.906682  \n\n--- GIN ---\n  conv_type  hidden_channels  latent_channels  dropout  use_batchnorm  \\\n0       gin               32                8      0.0          False   \n1       gin               32                4      0.0          False   \n2       gin               32               16      0.0          False   \n\n  sage_aggr      lr  num_epochs  batch_size  weight_decay   roc_auc  \n0      mean  0.0001          10         100       0.00001  0.890689  \n1      mean  0.0001          10         100       0.00001  0.886708  \n2      mean  0.0001          10         100       0.00001  0.886208  \n\n================ Best configuration overall (all conv_types) ================\n\nconv_type: sage\nhidden_channels: 128\nlatent_channels: 16\ndropout: 0.1\nuse_batchnorm: False\nsage_aggr: sum\nlr: 0.001\nnum_epochs: 10\nbatch_size: 100\nweight_decay: 1e-05\nroc_auc: 0.9113131494984047\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"id":"tBSVruL6OzOK"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"HIwUj8JMlwI0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Final scan","metadata":{"id":"RPjilveCEN4V"}},{"cell_type":"code","source":"# Cell 11: Example usage with hyperparameter control\n\n# Example 1: NO pT cut, compare GCN / SAGE / GIN on \"Znunu\"\n#            with dropout, SAGE aggregator, and BatchNorm turned on.\n# gae_conv_results_all_signals_nopt = run_gae_compare_convs_single(\n#     dataset_name=\"Znunu\",\n#     conv_types=(\"gcn\", \"sage\", \"gin\"),\n#     pt_threshold=None,\n#     num_epochs=20,\n#     hidden_channels=64,\n#     latent_channels=16,\n#     lr=1e-3,\n#     weight_decay=1e-5,\n#     dropout=0.2,\n#     random_state=42,\n#     sage_aggr=\"max\",\n#     use_batchnorm=True,\n# )\n\n# Example 2: ONE fixed pT cut (25 GeV), same hyperparams\n# gae_conv_results_all_signals_T25 = run_gae_compare_convs_single(\n#     dataset_name=\"all_signals\",\n#     conv_types=(\"gcn\", \"sage\", \"gin\"),\n#     pt_threshold=25,\n#     num_epochs=10,\n#     hidden_channels=64,\n#     latent_channels=16,\n#     lr=1e-3,\n#     weight_decay=1e-5,\n#     dropout=0.2,\n#     random_state=42,\n#     sage_aggr=\"mean\",\n#     use_batchnorm=True,\n# )\n\n# Example 3: FULL pT scan for all three convolutions (optional, slower)\n# gae_conv_results_all_signals_scan = run_gae_compare_convs_pt_scan(\n#     dataset_name=\"all_signals\",\n#     conv_types=(\"gcn\", \"sage\", \"gin\"),\n#     pt_thresholds=JET_PT_THRESHOLDS,\n#     num_epochs=10,\n#     batch_size=128,\n#     hidden_channels=64,\n#     latent_channels=16,\n#     lr=1e-3,\n#     weight_decay=1e-5,\n#     dropout=0.2,\n#     random_state=42,\n#     sage_aggr=\"mean\",\n#     use_batchnorm=True,\n# )\n","metadata":{"id":"ZnLOLes3-nIv"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hlt_data_all_tags = ['all_signals', 'HAHMggf', 'HNLeemu', 'HtoSUEP',\n'VBF_H125_a55a55_4b_ctau1_filtered', 'Znunu',\n'ggF_H125_a16a16_4b_ctau10_filtered', 'hh_bbbb_vbf_novhh_5fs_l1cvv1cv1']","metadata":{"id":"84W_JmFtD3ZS"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for tag in hlt_data_all_tags:\n  gae_conv_results_all_signals_nopt = run_gae_compare_convs_single(\n    dataset_name=tag,\n    conv_types=(\"gcn\", \"sage\", \"gin\"),\n    pt_threshold=None,\n    num_epochs=20,\n    hidden_channels=64,\n    latent_channels=16,\n    lr=1e-3,\n    weight_decay=1e-5,\n    dropout=0.2,\n    random_state=42,\n    sage_aggr=\"mean\",\n    use_batchnorm=True,\n)","metadata":{"collapsed":true,"id":"Yy8o-mAJD4UL","jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for tag in hlt_data_all_tags:\n  gae_conv_results_all_signals_scan = run_gae_compare_convs_pt_scan(\n    dataset_name=tag,\n    conv_types=(\"gcn\", \"sage\", \"gin\"),\n    pt_thresholds=JET_PT_THRESHOLDS,\n    num_epochs=10,\n    batch_size=128,\n    hidden_channels=64,\n    latent_channels=16,\n    lr=1e-3,\n    weight_decay=1e-5,\n    dropout=0.2,\n    random_state=42,\n    sage_aggr=\"mean\",\n    use_batchnorm=True,\n)","metadata":{"id":"R6xGBhUCWDOM"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"ugrflE5NA2gt"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"c4JFKU9qA2PF"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**To load the model in a different notebook:**\n\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nimport os\n\n# --- Step 1: Mount Google Drive (if your model is saved there) ---\n# This is necessary if your model is in MyDrive. Skip if not applicable.\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# --- Step 2: Re-define the GraphAutoEncoder class (EXACTLY as it was trained) ---\n# This class definition must match the one from cell TgGOtSB2GnKe when the model was trained.\nclass GraphAutoEncoder(nn.Module):\n    def __init__(\n        self,\n        in_channels=8,\n        hidden_channels1=64, # These parameters are part of the __init__ signature\n        hidden_channels2=32, # but were not used in the 'shallow' model's layers.\n        latent_channels=16,\n    ):\n        super().__init__()\n\n        # --- Encoder GNN: 1 layer ---\n        self.conv1 = GCNConv(in_channels, latent_channels)\n\n        # --- Decoder MLP: 1 layer ---\n        self.dec_lin1 = nn.Linear(latent_channels, in_channels)\n\n    def encode(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        z = F.relu(x)  # Latent node embeddings\n        return z\n\n    def decode(self, z):\n        x_hat = self.dec_lin1(z)\n        return x_hat\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        z = self.encode(x, edge_index)\n        x_hat = self.decode(z)\n        return x_hat, z\n\n# Define the device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Step 3: Instantiate the model with the EXACT parameters used during training ---\n# These values must match those found in cell `5mQeb_tVI3dq` of the training notebook.\nTRAINING_IN_CHANNELS = 8\nTRAINING_HIDDEN_CHANNELS1 = 128\nTRAINING_HIDDEN_CHANNELS2 = 64\nTRAINING_LATENT_CHANNELS = 4\n\nloaded_model = GraphAutoEncoder(\n    in_channels=TRAINING_IN_CHANNELS,\n    hidden_channels1=TRAINING_HIDDEN_CHANNELS1,\n    hidden_channels2=TRAINING_HIDDEN_CHANNELS2,\n    latent_channels=TRAINING_LATENT_CHANNELS,\n).to(device)\n\n# --- Step 4: Define the path to your saved model file ---\n# IMPORTANT: You MUST update this path to where YOUR model is saved.\n# The example path below uses the last saved path from this notebook.\nmodel_save_path = \"/content/drive/MyDrive/gae_experiments/exp_20251125_100949/gae_model_state_dict.pth\"\n\n# --- Step 5 & 6: Load the state dictionary and set to evaluation mode ---\nif not os.path.exists(model_save_path):\n    print(f\"Error: Model file not found at {model_save_path}\")\n    print(\"Please ensure Google Drive is mounted and the path is correct.\")\nelse:\n    # Load the state dictionary, mapping to the current device\n    loaded_state_dict = torch.load(model_save_path, map_location=device)\n\n    # Apply the loaded state dictionary to the model\n    loaded_model.load_state_dict(loaded_state_dict)\n\n    # Set the model to evaluation mode (important for inference)\n    loaded_model.eval()\n\n    print(f\"Model successfully loaded from: {model_save_path}\")\n    print(\"Loaded Model Architecture:\")\n    print(loaded_model)\n\n    # You can now use `loaded_model` for predictions in your new notebook.\n```\n\n","metadata":{"id":"Ilh0OUYphpZm"}},{"cell_type":"code","source":"","metadata":{"id":"BAHZrucpSqCC"},"outputs":[],"execution_count":null}]}